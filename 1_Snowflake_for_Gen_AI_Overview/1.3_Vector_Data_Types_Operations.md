[< Cortex AI Features](./1.2_Snowflake_Cortex_AI_Features.md) | **1.3 Vector Data Types & Operations** | [Domain 2 >](../2_Snowflake_Gen_AI_and_LLM_Functions/README.md)
***

# 1.3 Vector Data Types & Operations

## Overview

Snowflake's native VECTOR data type enables efficient storage and processing of vector embeddings for AI/ML applications, supporting similarity searches, clustering, and retrieval-augmented generation (RAG) patterns.

## Vector Data Type

### Declaration Syntax
```sql
-- Vector with specific dimensions and data type
VECTOR(element_type, dimension)

-- Common patterns
VECTOR(FLOAT, 768)     -- For most embedding models
VECTOR(FLOAT, 1024)    -- For larger embedding models
VECTOR(INT, 512)       -- For integer-based vectors
```

### Supported Element Types
- **FLOAT**: Most common for embeddings (default precision)
- **INT**: For integer vectors
- **NUMBER**: For high-precision requirements

## Creating Tables with Vectors

### Basic Table Creation
```sql
CREATE TABLE document_embeddings (
    id INTEGER,
    document_text STRING,
    embedding VECTOR(FLOAT, 768),
    metadata VARIANT
);
```

### With Constraints
```sql
CREATE TABLE product_vectors (
    product_id INTEGER PRIMARY KEY,
    product_name STRING NOT NULL,
    description_embedding VECTOR(FLOAT, 1024) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);
```

## Generating Vector Embeddings

### Using Cortex Embedding Functions
```sql
-- 768-dimensional embeddings
INSERT INTO document_embeddings (id, document_text, embedding)
SELECT 
    id,
    text_content,
    SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', text_content)
FROM source_documents;

-- 1024-dimensional embeddings
INSERT INTO large_embeddings (id, text, embedding)
SELECT 
    id,
    content,
    SNOWFLAKE.CORTEX.EMBED_TEXT_1024('multilingual-e5-large', content)
FROM multilingual_content;
```

### Available Embedding Models
```sql
-- 768-dimensional models
SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m')
SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5')
SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2')

-- 1024-dimensional models
SNOWFLAKE.CORTEX.EMBED_TEXT_1024('multilingual-e5-large')
SNOWFLAKE.CORTEX.EMBED_TEXT_1024('voyage-multilingual-2')
SNOWFLAKE.CORTEX.EMBED_TEXT_1024('nv-embed-qa-4')
```

## Vector Operations & Functions

### Similarity Functions

#### Cosine Similarity
```sql
-- Most common for text embeddings
SELECT 
    doc_id,
    document_text,
    VECTOR_COSINE_SIMILARITY(
        embedding,
        SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'search query')
    ) as similarity_score
FROM document_embeddings
ORDER BY similarity_score DESC
LIMIT 10;
```

#### L2 Distance (Euclidean)
```sql
-- Distance-based similarity (lower = more similar)
SELECT 
    product_id,
    VECTOR_L2_DISTANCE(
        product_embedding,
        SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'target product')
    ) as distance
FROM product_embeddings
ORDER BY distance ASC
LIMIT 5;
```

#### Inner Product
```sql
-- Dot product similarity
SELECT 
    item_id,
    VECTOR_INNER_PRODUCT(
        item_vector,
        user_preference_vector
    ) as relevance_score
FROM items
ORDER BY relevance_score DESC;
```

### Vector Aggregation

#### Centroid Calculation
```sql
-- Calculate average vector for clustering
SELECT 
    category,
    VECTOR_L2_NORMALIZE(
        ARRAY_AGG(embedding)::VECTOR(FLOAT, 768)
    ) as category_centroid
FROM product_embeddings
GROUP BY category;
```

## Indexing Strategies

### Standard Indexing
```sql
-- Create index on vector column for performance
CREATE INDEX idx_embedding ON document_embeddings(embedding);
```

### Clustered Tables
```sql
-- Cluster table for better similarity search performance
CREATE TABLE clustered_embeddings (
    id INTEGER,
    text STRING,
    embedding VECTOR(FLOAT, 768)
) CLUSTER BY (embedding);
```

## RAG Implementation Patterns

### Document Chunking & Embedding
```sql
-- Process documents into searchable chunks
WITH document_chunks AS (
    SELECT 
        doc_id,
        chunk_id,
        chunk_text,
        SNOWFLAKE.CORTEX.EMBED_TEXT_768(
            'snowflake-arctic-embed-m-v1.5', 
            chunk_text
        ) as chunk_embedding
    FROM (
        SELECT 
            doc_id,
            ROW_NUMBER() OVER (PARTITION BY doc_id ORDER BY position) as chunk_id,
            SUBSTR(full_text, position, 1000) as chunk_text
        FROM document_chunks_raw
    )
)
INSERT INTO vector_store SELECT * FROM document_chunks;
```

### Similarity Search with Metadata Filtering
```sql
-- RAG retrieval with filtering
SELECT 
    doc_id,
    chunk_text,
    similarity_score,
    metadata:author::STRING as author,
    metadata:date::DATE as doc_date
FROM (
    SELECT 
        doc_id,
        chunk_text,
        metadata,
        VECTOR_COSINE_SIMILARITY(
            chunk_embedding,
            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', ?)
        ) as similarity_score
    FROM vector_store
    WHERE metadata:category::STRING = 'technical_docs'
)
WHERE similarity_score > 0.7
ORDER BY similarity_score DESC
LIMIT 5;
```

### Context Assembly for LLM
```sql
-- Combine retrieved chunks for LLM context
WITH relevant_chunks AS (
    SELECT 
        chunk_text,
        VECTOR_COSINE_SIMILARITY(embedding, ?) as score
    FROM document_embeddings
    ORDER BY score DESC
    LIMIT 3
),
assembled_context AS (
    SELECT LISTAGG(chunk_text, '\n\n') as context
    FROM relevant_chunks
)
SELECT SNOWFLAKE.CORTEX.COMPLETE(
    'llama3.1-70b',
    CONCAT(
        'Based on this context: ', context,
        '\n\nQuestion: ', ?,
        '\n\nAnswer:'
    )
) as llm_response
FROM assembled_context;
```

## Performance Optimization

### Dimension Reduction
```sql
-- Reduce vector dimensions for faster processing
CREATE TABLE reduced_vectors AS
SELECT 
    id,
    text,
    -- Take first 512 dimensions
    ARRAY_SLICE(embedding::ARRAY, 0, 512)::VECTOR(FLOAT, 512) as reduced_embedding
FROM full_embeddings;
```

### Batch Processing
```sql
-- Process embeddings in batches
INSERT INTO embeddings_table
SELECT 
    id,
    text,
    SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', text)
FROM source_table
WHERE batch_id = 1;  -- Process in chunks
```

### Approximation Techniques
```sql
-- Use approximate similarity for large datasets
SELECT *
FROM vector_table
WHERE VECTOR_COSINE_SIMILARITY(embedding, query_vector) > 0.8  -- Pre-filter
ORDER BY VECTOR_COSINE_SIMILARITY(embedding, query_vector) DESC
LIMIT 20;
```

## Multi-Modal Vectors

### Combining Text and Metadata Vectors
```sql
-- Weighted combination of different vector types
SELECT 
    id,
    (0.7 * VECTOR_COSINE_SIMILARITY(text_embedding, query_text_embedding) +
     0.3 * VECTOR_COSINE_SIMILARITY(metadata_embedding, query_meta_embedding)) as combined_score
FROM multi_modal_table
ORDER BY combined_score DESC;
```

## Common Use Cases

### Semantic Search
```sql
CREATE OR REPLACE FUNCTION semantic_search(query STRING, limit_count INTEGER)
RETURNS TABLE (id INTEGER, text STRING, score FLOAT)
AS $$
    SELECT 
        id,
        document_text,
        VECTOR_COSINE_SIMILARITY(
            embedding,
            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', query)
        ) as score
    FROM document_embeddings
    ORDER BY score DESC
    LIMIT limit_count
$$;
```

### Recommendation Systems
```sql
-- Find similar products
WITH user_preferences AS (
    SELECT AVG(product_embedding) as user_vector
    FROM user_interactions ui
    JOIN product_embeddings pe ON ui.product_id = pe.product_id
    WHERE ui.user_id = ?
)
SELECT 
    pe.product_id,
    pe.product_name,
    VECTOR_COSINE_SIMILARITY(pe.product_embedding, up.user_vector) as relevance
FROM product_embeddings pe
CROSS JOIN user_preferences up
ORDER BY relevance DESC
LIMIT 10;
```

### Duplicate Detection
```sql
-- Find duplicate or near-duplicate content
SELECT 
    a.id as doc1_id,
    b.id as doc2_id,
    VECTOR_COSINE_SIMILARITY(a.embedding, b.embedding) as similarity
FROM document_embeddings a
JOIN document_embeddings b ON a.id < b.id
WHERE VECTOR_COSINE_SIMILARITY(a.embedding, b.embedding) > 0.95;
```

## Study Questions

1. **What is the maximum dimension size for VECTOR data type in Snowflake?**
   - A) 1024
   - B) 2048
   - C) 16384
   - D) No documented limit

2. **Which similarity function is most commonly used for text embeddings?**
   - A) VECTOR_L2_DISTANCE
   - B) VECTOR_COSINE_SIMILARITY
   - C) VECTOR_INNER_PRODUCT
   - D) VECTOR_EUCLIDEAN_DISTANCE

3. **What embedding model produces 768-dimensional vectors?**
   - A) multilingual-e5-large
   - B) snowflake-arctic-embed-m
   - C) nv-embed-qa-4
   - D) voyage-multilingual-2

**Answers: 1-C, 2-B, 3-B**

***
[< Cortex AI Features](./1.2_Snowflake_Cortex_AI_Features.md) | **1.3 Vector Data Types & Operations** | [Domain 2 >](../2_Snowflake_Gen_AI_and_LLM_Functions/README.md) 